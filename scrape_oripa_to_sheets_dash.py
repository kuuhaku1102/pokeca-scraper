import os
import time
import pymysql
from urllib.parse import urljoin
from playwright.sync_api import sync_playwright

# -----------------------------
# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ DB æ¥ç¶šè¨­å®šã‚’èª­ã¿è¾¼ã‚€
# -----------------------------
DB_HOST = os.environ.get("DB_HOST")
DB_NAME = os.environ.get("DB_NAME")
DB_USER = os.environ.get("DB_USER")
DB_PASS = os.environ.get("DB_PASS")
DB_PORT = int(os.environ.get("DB_PORT", 3306))

# -----------------------------
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡è¨­å®š
# -----------------------------
BASE_URL = "https://oripa-dash.com"
TARGET_URL = "https://oripa-dash.com/user/packList"

# -----------------------------
# MySQL ã«ä¿å­˜
# -----------------------------
def save_to_db(rows):
    if not rows:
        print("ğŸ“­ DBæ›´æ–°ãªã—ï¼ˆæ–°è¦ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
        return

    conn = pymysql.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASS,
        db=DB_NAME,
        port=DB_PORT,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
    )

    try:
        with conn.cursor() as cur:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒãªã‘ã‚Œã°ä½œæˆ
            cur.execute("""
                CREATE TABLE IF NOT EXISTS wp_scrapes (
                    id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
                    title VARCHAR(255),
                    image_url TEXT,
                    detail_url TEXT,
                    pt VARCHAR(50),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE KEY uniq_url (detail_url(255))
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
            """)

            sql = """
                INSERT INTO wp_scrapes (title, image_url, detail_url, pt)
                VALUES (%s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                  title=VALUES(title),
                  image_url=VALUES(image_url),
                  pt=VALUES(pt),
                  created_at=CURRENT_TIMESTAMP
            """
            cur.executemany(sql, rows)
        conn.commit()
        print(f"ğŸ’¾ DBã« {len(rows)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
    finally:
        conn.close()

# -----------------------------
# Playwright ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# -----------------------------
def scrape_dash():
    print("ğŸ” oripa-dash.com ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    rows = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
        page = browser.new_page()

        try:
            page.goto(TARGET_URL, timeout=60000, wait_until="load")
            page.wait_for_timeout(4000)
        except Exception as e:
            print(f"ğŸ›‘ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            browser.close()
            return rows

        # å„ã‚ªãƒªãƒ‘æƒ…å ±ã‚’å–å¾—
        items = page.query_selector_all(".packList__item")
        print(f"ğŸ“¦ {len(items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¤œå‡º")

        for item in items:
            title = item.get_attribute("data-pack-name") or ""
            pack_id = item.get_attribute("data-pack-id") or ""
            img_tag = item.query_selector("img.packList__item-thumbnail")
            img_url = img_tag.get_attribute("src") if img_tag else ""
            if img_url.startswith("/"):
                img_url = urljoin(BASE_URL, img_url)

            pt_tag = item.query_selector(".packList__pt-txt")
            pt_text = pt_tag.inner_text().strip() if pt_tag else ""

            detail_url = f"{BASE_URL}/user/itemDetail?id={pack_id}" if pack_id else TARGET_URL
            rows.append([title, img_url, detail_url, pt_text])

        browser.close()
    print(f"âœ… {len(rows)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—å®Œäº†")
    return rows

# -----------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------
def main():
    start = time.time()
    rows = scrape_dash()
    if not rows:
        print("ğŸ“­ æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    save_to_db(rows)
    print(f"ğŸ å®Œäº†ï¼å‡¦ç†æ™‚é–“: {round(time.time() - start, 2)} ç§’")

if __name__ == "__main__":
    main()
