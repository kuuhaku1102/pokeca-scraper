import time
import os
import base64
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials

# ======== èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ ========
CREDENTIALS_FILE = "credentials.json"
encoded_json = os.environ.get("GSHEET_JSON", "")
if not encoded_json:
    raise Exception("âŒ GSHEET_JSON ãŒç©ºã§ã™ã€‚base64æ–‡å­—åˆ—ã‚’ GitHub Secrets ã«ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")

with open(CREDENTIALS_FILE, "wb") as f:
    f.write(base64.b64decode(encoded_json))

# ======== Google Sheets èªè¨¼ ========
scopes = ["https://www.googleapis.com/auth/spreadsheets"]
credentials = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=scopes)
client = gspread.authorize(credentials)
sheet = client.open_by_url("https://docs.google.com/spreadsheets/d/11agq4oxQxT1g9ZNw_Ad9g7nc7PvytHr1uH5BSpwomiE/edit").worksheet("dash")

# ======== Chromeèµ·å‹•ï¼ˆheadlessï¼‰ ========
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# ======== oripa-dash ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ï¼‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« ========
print("ğŸ” oripa-dash.com ã‚’èª­ã¿è¾¼ã¿ä¸­...")
driver.get("https://oripa-dash.com/user/packList")
time.sleep(2)

last_height = driver.execute_script("return document.body.scrollHeight")
scroll_attempts = 0
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.5)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        scroll_attempts += 1
        if scroll_attempts >= 3:
            break
    else:
        scroll_attempts = 0
    last_height = new_height

# ======== HTMLãƒ‘ãƒ¼ã‚¹ & ãƒ‡ãƒ¼ã‚¿æŠ½å‡º ========
soup = BeautifulSoup(driver.page_source, "html.parser")
items = soup.select(".packList__item")

result = [["ã‚¿ã‚¤ãƒˆãƒ«", "ç”»åƒURL", "URL"]]
for item in items:
    title = item.get("data-pack-name", "No Title").strip()
    pack_id = item.get("data-pack-id", "").strip()
    url = f"https://oripa-dash.com/user/packDetail/{pack_id}" if pack_id else ""

    img_tag = item.select_one("img.packList__item-thumbnail")
    img_url = img_tag.get("src") if img_tag else ""
    if img_url.startswith("/"):
        img_url = "https://oripa-dash.com" + img_url

    result.append([title, img_url, url])

print(f"ğŸŸ¢ å–å¾—ä»¶æ•°: {len(result)-1} ä»¶")

# ======== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ ========
sheet.clear()
sheet.append_rows(result)
print("âœ… Google Sheets ã«ä¿å­˜å®Œäº†")

driver.quit()
